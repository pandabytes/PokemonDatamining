{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Decision Tree & Naive Bayes to classify Legendary Pokemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas_profiling as pdp\n",
    "# from sklearn.feature_extraction import FeatureHasher\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "import utils as ut\n",
    "import decisionTree as dt\n",
    "import naiveBayes as nb\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set packages options\n",
    "# np.set_printoptions(threshold=np.nan)\n",
    "pd.set_option(\"display.max_columns\", 600)\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TypeColorMappings = {\"Water\": \"#6890F0\", \"Fire\": \"#F08030\", \"Grass\": \"#78C850\",\n",
    "                     \"Dark\": \"#705848\", \"Electric\": \"#F8D030\", \"Flying\": \"#A890F0\",\n",
    "                     \"Normal\": \"#A8A878\", \"Fighting\": \"#C03028\", \"Poison\": \"#A040A0\",\n",
    "                     \"Ground\": \"#E0C068\", \"Psychic\": \"#F85888\", \"Rock\": \"#B8A038\", \n",
    "                     \"Ice\": \"#98D8D8\", \"Bug\": \"#A8B820\", \"Dragon\": \"#7038F8\", \n",
    "                     \"Ghost\": \"#705898\", \"Steel\": \"#B8B8D0\", \"Fairy\": \"#EE99AC\"}\n",
    "\n",
    "# DropColumns = [\"Pokedex#\", \"Name\", \"Type 1\", \"Type 2\", \"Generation\", \"Ability 1\", \"Ability 2\", \"Ability 3\", \n",
    "#                \"EggGroup 1\", \"EggGroup 2\", \"Category\", \"Height (m)\", \"Weight (kg)\"]\n",
    "\n",
    "DropColumns = [\"Pokedex#\", \"Name\", \"Generation\", \"Category\"]\n",
    "\n",
    "def getColorList(typeCounts):\n",
    "    assert type(typeCounts) == pd.core.series.Series, \"Argument must be a Series object\"\n",
    "    return [TypeColorMappings[pokemonType] for pokemonType in typeCounts.index]\n",
    "\n",
    "def getDistinctValues(dataFrame, columnName, sep):\n",
    "    result = []\n",
    "    for value in dataFrame[columnName].value_counts().index:\n",
    "        result += value.split(sep)\n",
    "    return set(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the Pokemon data\n",
    "fileName = r'.\\Pokemon_Cleaned.tsv'\n",
    "columnTypes = {\"Name\": str, \"Category\": str, \"Type 1\": str, \"Type 2\": str, \n",
    "               \"Ability 1\": str, \"Ability 2\": str, \"Ability 3\": str, \"Group\": str}\n",
    "data = pd.read_csv(fileName, header=0, sep='\\t', dtype=columnTypes)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphs to visualize and understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Group occurences to see the distributions\n",
    "groupFig, groupAxes = plt.subplots(nrows=1, ncols=1)\n",
    "groupCounts = data[\"Group\"].value_counts()\n",
    "groupAxe = groupCounts.plot(title=\"Group\", kind=\"bar\")\n",
    "groupAxe.set(xlabel=\"Group Types\", ylabel=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Box plot Total & Group\n",
    "data.boxplot(\"Total\", \"Group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> According to the boxplot above, we can see that despite having a small quanity in size, Legendary Pokemon has the highest Total stats compared to the other 2 groups.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedData = data.drop(DropColumns, axis=1)\n",
    "# dummies = pd.get_dummies(data[[\"Type 1\", \"Type 2\", \"Ability 1\", \"Ability 2\", \"Ability 3\", \"EggGroup 1\", \"EggGroup 2\"]])\n",
    "# dataWithDummies = pd.concat([dummies, reducedData], axis=1, join_axes=[reducedData.index])\n",
    "\n",
    "# x = dataWithDummies.loc[:, dataWithDummies.columns != \"Group\"]\n",
    "# y = dataWithDummies.loc[:, \"Group\"]\n",
    "\n",
    "# Scale the x data\n",
    "#x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "width, height = principalComponents.shape\n",
    "principalDf = pd.DataFrame(data=principalComponents, columns=[\"pca %d\" % i for i in range(1, height + 1)])\n",
    "pcaData = pd.concat([principalDf, dataWithDummies[[\"Group\"]]], axis=1)\n",
    "\n",
    "xTrain = pcaData.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Plot Type 1 and Type 2 occurences to see the distributions\n",
    "# typeFig, typeAxes = plt.subplots(nrows=3, ncols=1)\n",
    "# typeFig.subplots_adjust(top=3)\n",
    "\n",
    "# type1Counts = data[\"Type 1\"].value_counts()\n",
    "# type2Counts = data[\"Type 2\"].value_counts().drop(\"None\")\n",
    "# typesCounts = type1Counts.add(type2Counts, fill_value=0)\n",
    "\n",
    "# type1Counts.plot(title=\"Type 1 Occurrences\", kind=\"bar\", ax=typeAxes[0], color=getColorList(type1Counts))\n",
    "# type2Counts.plot(title=\"Type 2 Occurrences\", kind=\"bar\", ax=typeAxes[1], color=getColorList(type2Counts))\n",
    "# typesCounts.plot(title=\"Type 1 + 2 Occurrences\", kind=\"bar\", ax=typeAxes[2], color=getColorList(typesCounts))\n",
    "\n",
    "# fh = FeatureHasher(n_features=2, input_type=\"string\")\n",
    "# hashedFeature1 = fh.fit_transform(data[\"Type 1\"])\n",
    "# hashedFeature2 = fh.fit_transform(data[\"Type 2\"])\n",
    "\n",
    "# x = pd.concat([data[[\"Name\", \"Type 1\", \"Type 2\", \"Category\"]], pd.DataFrame(hashedFeature1.toarray())], axis=1)\n",
    "# x = pd.concat([x, pd.DataFrame(hashedFeature2.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Group\"\n",
    "reducedData = data.drop(DropColumns, axis=1)\n",
    "training, test = ut.splitData(target, reducedData, 0.60)\n",
    "k = min(training[\"Group\"].value_counts())\n",
    "kTrainings, kTests = ut.kFoldCrossValidation(k, training, True, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtree = dt.DecisionTree(target, 5)\n",
    "dtree.train(training)\n",
    "dtPred = dtree.classify(test.drop([target], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtree.countTreeDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.countLeafNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtMatrix = ut.buildConfusionMatrix(dtPred, test[target], reducedData[target].unique())\n",
    "dtPrecisions, dtRecalls = ut.getPrecisionsAndRecalls(dtMatrix, reducedData[target].unique())\n",
    "dtFScores = ut.computeFScores(dtPrecisions, dtRecalls)\n",
    "\"Error {0:.2f}%\".format(ut.computeError(dtPred, test[\"Group\"]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "matrices = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "fScores = []\n",
    "\n",
    "for kTraining, kTest in zip(kTrainings, kTests):\n",
    "    dtree.train(kTraining)\n",
    "    kPred = dtree.classify(kTest.drop([target], axis=1))\n",
    "    \n",
    "    kMatrix = ut.buildConfusionMatrix(kPred, kTest[target], reducedData[target].unique())\n",
    "    kPrecisions, kRecalls = ut.getPrecisionsAndRecalls(kMatrix, reducedData[target].unique())\n",
    "    kFScores = ut.computeFScores(kPrecisions, kRecalls)\n",
    "    error = ut.computeError(kPred, kTest[\"Group\"])\n",
    "    \n",
    "    errors.append(error)\n",
    "    matrices.append(kMatrix)\n",
    "    precisions.append(kPrecisions)\n",
    "    recalls.append(kRecalls)\n",
    "    fScores.append(kFScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nBayes = nb.NaiveBayes(target)\n",
    "nBayes.train(training, reducedData[target].unique())\n",
    "nbPred = nBayes.classify(test.drop([target], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbMatrix = ut.buildConfusionMatrix(nbPred, test[target], reducedData[target].unique())\n",
    "nbPrecisions, nbRecalls = ut.getPrecisionsAndRecalls(nbMatrix, reducedData[target].unique())\n",
    "nbFScores = ut.computeFScores(nbPrecisions, nbRecalls)\n",
    "nbSens, nbSpec = ut.getSensitivityAndSpecifiicy(nbMatrix, reducedData[target].unique())\n",
    "\"Error: {0:.2f}%\".format(ut.computeError(nbPred, test[\"Group\"]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:1,\"Group\"].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nberrors = []\n",
    "nbmatrices = []\n",
    "nbprecisions = []\n",
    "nbrecalls = []\n",
    "nbfScores = []\n",
    "\n",
    "for kTraining, kTest in zip(kTrainings, kTests):\n",
    "    nBayes.train(kTraining,  reducedData[target].unique())\n",
    "    kPred = nBayes.classify(kTest.drop([target], axis=1))\n",
    "    \n",
    "    kMatrix = ut.buildConfusionMatrix(kPred, kTest[target], reducedData[target].unique())\n",
    "    kPrecisions, kRecalls = ut.getPrecisionsAndRecalls(kMatrix, reducedData[target].unique())\n",
    "    kFScores = ut.computeFScores(kPrecisions, kRecalls)\n",
    "    error = ut.computeError(kPred, kTest[\"Group\"])\n",
    "    \n",
    "    nberrors.append(error)\n",
    "    nbmatrices.append(kMatrix)\n",
    "    nbprecisions.append(kPrecisions)\n",
    "    nbrecalls.append(kRecalls)\n",
    "    nbfScores.append(kFScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "nBayes = nb.NaiveBayes(target)\n",
    "sens = []\n",
    "specs = []\n",
    "\n",
    "for i in range(k):\n",
    "    training, test = ut.splitData(target, reducedData, 0.60)    \n",
    "    nBayes.train(training, reducedData[target].unique())\n",
    "    nbPred = nBayes.classify(test.drop([target], axis=1))\n",
    "    \n",
    "    nbMatrix = ut.buildConfusionMatrix(nbPred, test[target], reducedData[target].unique())\n",
    "    nbSens, nbSpec = ut.getSensitivityAndSpecifiicy(nbMatrix, reducedData[target].unique())\n",
    "    \n",
    "    sens.append(nbSens)\n",
    "    specs.append(nbSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinarySens = [sens[i][\"Ordinary\"] for i in range(k)]\n",
    "ordinarySpecs = sorted([specs[i][\"Ordinary\"] for i in range(k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Legendary\"\n",
    "s = sorted([(sens[i][t], specs[i][t]) for i in range(k)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda x: 1- x, [i[1] for i in s])), [i[0] for i in s], 'bo-', label=\"Ordinary ROC Curve\")\n",
    "plt.xlabel(\"1 - Specificity\")\n",
    "plt.ylabel(\"Sensitivy\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = ut.splitData(target, reducedData, 0.60)    \n",
    "nBayes.train(training, reducedData[target].unique())\n",
    "nbPred = nBayes.classify(test.drop([target], axis=1))\n",
    "\n",
    "nbMatrix = ut.buildConfusionMatrix(nbPred, test[target], reducedData[target].unique())\n",
    "nbSens, nbSpec = ut.getSensitivityAndSpecifiicy(nbMatrix, reducedData[target].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(map(lambda x: 1 if x == \"Ordinary\" else 0, nbPred.values))\n",
    "a = list(map(lambda x: 1 if x == \"Ordinary\" else 0, test[\"Group\"]))\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(a, p)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_true=test[\"Group\"], pos_label=\"Ordinary\")\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-nbSpec[\"Ordinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbSens[\"Ordinary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[:190, :]\n",
    "test = data.loc[850:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.train(train)\n",
    "pred = dtree.classify(test.drop([\"Group\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ut.buildConfusionMatrix(pred, test[\"Group\"], data[\"Group\"].unique())\n",
    "p, r = ut.getPrecisionsAndRecalls(m, data[target].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, j in skf.split(training, training[\"Group\"]):\n",
    "    print(\"***Training:\\n\", training.iloc[i][\"Group\"].value_counts())\n",
    "    print(training.iloc[i][training.iloc[i][\"Group\"] == \"Ultra Beast\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ut._kFoldSample(10, data, \"Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(10):\n",
    "    print(u[i][\"Group\"].value_counts())\n",
    "    l += list(u[i].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (a,b),(c,d) in zip(nbprecisions[3].items(), nbprecisions[2].items()):\n",
    "    print(a, b, \"-----\", c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbprecisions[2].items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPrecisionRecall(precisions, recalls):\n",
    "    assert len(precisions) == len(recalls), \"Length of precisions and recalls must match\"\n",
    "    assert precisions.keys() == recalls.keys(), \"Keys in precisions and recalls must match\"\n",
    "    \n",
    "    for label in precisions.keys():\n",
    "        pValue = precisions[label]\n",
    "        rValue = recalls[label]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratios = [0.2, 0.4, 0.6, 0.8]\n",
    "dt = DecisionTree(\"Group\")\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for ratio in ratios:\n",
    "    print(\"************ Split ratio: \", ratio)\n",
    "    training, test = splitData(data, ratio)\n",
    "    node = dt.train(training)\n",
    "    predictions = dt.classify(test, node)\n",
    "    error = computeError(predictions, test[dt.targetFeature])\n",
    "    \n",
    "    x.append(len(training))\n",
    "    y.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'bo-', label=\"With categorical and continuous features\")\n",
    "plt.xlabel(\"Trainging Set size\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data profile\n",
    "profile = pdp.ProfileReport(dataWithDummies)\n",
    "profile.to_file(\"Profile.html\")\n",
    "profile = None\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
